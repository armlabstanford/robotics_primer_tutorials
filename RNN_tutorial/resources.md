Useful resources that we've found while preparing for this talk:

# Probability:

## Probability Distribution Estimation

- [[Link]](https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9) MAP vs. MLE vs. Bayesian Inference

## Recursive Bayesian Estimation:

- [[Link]](https://people.csail.mit.edu/mrub/talks/filtering.pdf) MIT slides on filtering

## RNN:

- [[Link]](https://www.youtube.com/watch?v=LHXXI4-IEns&feature=youtu.be) Explanatory video on how RNN works

### RNN Training:

- [[Link]](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/) Loss function considerations during RNN training
- [[Link]](https://gombru.github.io/2018/05/23/cross_entropy_loss/) More details on multiclass and binary entropy loss
- [[Link]](https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a) An easier-to-read article on cross entropy

## LSTM:

- [[Link]](https://www.youtube.com/watch?v=8HyCNIVRbSU&feature=youtu.be) Explanatory video on how LSTM works

## Attention:

- [[Link]](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0) Overview article describing the drawbacks of RNN/LSTM and the emergence of attention-based models.
- [[Link]](https://towardsdatascience.com/attention-in-neural-networks-e66920838742) Attention in Neural Networks
- [[Link]](https://towardsdatascience.com/memory-attention-sequences-37456d271992) Memory, Attention, Sequences
- [[Link]](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) Explains different types of Attention really well
- [[Link]](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#0458) Detailed breakdown on how global Attention works!
- [[Link]](https://towardsdatascience.com/attention-networks-c735befb5e9f) Detailed breakdown on Transformers

## Transformers:

- [[Link]](https://towardsdatascience.com/transformers-141e32e69591) Transformers
